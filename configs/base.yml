epochs: 1
clip: 1

samples: 3
tokenizer_max_length: 512 # sequence length
task: tbd

method: baye_by_backprop

model:
  backbone_network: distilbert-base-uncased
  projection_size: 200
  projection_hidden: 200
  dropout_p: .2

dataset:
  train_dataset: 
    name: WikiText2
    root: ./dataset/wikitext2
  ood_detection_dataset: 
    name: EnWik9

loader:
  num_workers: 0
  batch_size: 2
  pin_memory: true
  drop_last: true

optim:
  name: adamw
  lr: 1e-3

env:
  fp16: true
  n_gpu: 1

# scheduler:
#   name: cycle
#   decay: lin, cos