debug: False
epochs: 1
clip: 1

freeze_backbone: True
samples: 8
noise_samples: 10
lr: 1e-3
tokenizer_max_length: 200 # sequence length
task: tbd

method: baye_by_backprop

model:
  backbone_network: distilbert-base-uncased
  projection_size: 718
  projection_hidden: 718
  dropout_p: .1

dataset:
  train_dataset: 
    name: WikiText2
    root: ./dataset/wikitext2
  ood_detection_dataset: 
    name: EnWik9

loader:
  num_workers: 0
  batch_size: 32
  pin_memory: false
  drop_last: true

optim:
  name: adamw
  lr: 1e-3

env:
  fp16: false
  n_gpu: 4
  distributed: True

# scheduler:
#   name: cycle
#   decay: lin, cos