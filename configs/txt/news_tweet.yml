#include txt.yml

eval_train_epoch_fraction: arange(2., 5.)

# In this experience, we just analyze the outputs of different pre-trained models without re-training them
trainer: IdentityTrainer

model:
  dropout_p: .1           # N/A
  projection_hidden: 800  # N/A

  projector: simple
  projection_size: 64, 512, 256
  # backbone_network: '"facebook/bart-base", "roberta-base", "albert-base-v2", "bert-base-uncased", "distilbert-base-uncased"'
  backbone_network: '"distilroberta-base","roberta-base", "roberta-large","roberta-large-mnli","albert-base-v1","albert-large-v1","albert-base-v2", "albert-large-v2","t5-small","t5-base","t5-large,""facebook/bart-base","facebook/bart-large","reformer-enwik8","allenai/longformer-base-4096","distilbert-base-uncased","bert-base-uncased", "bert-large-uncased", "bert-base-multilingual-uncased", "bert-large-cased-whole-word-masking","openai-gpt","distilgpt2","gpt2","gpt2-medium","gpt2-large","xlnet-base-cased","xlnet-large-cased","xlm-mlm-en-2048","xlm-mlm-17-1280"'

dataset:
  max_classes: 5

  input_position: 1
  label_position: 0
  
  ood_input_position: 0
  ood_label_position: 1

  train_dataset: 
    name: '"AG_NEWS", "newspop"'
    root: ./dataset
  ood_detection_dataset: 
    name: rotten_tomatoes
    root: ./dataset

